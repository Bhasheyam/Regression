---
title: "Linear Regression on CPU-Performance Dataset"
output: github_document
author: Bhasheyam Krishnan
---
Loading the dataset
```{r}
cpu = read.csv(file = "CPUPerformance.csv")
head(cpu)
```


```{r}
anyNA(cpu)
```



the dataset has no NA and missing values
```{r}
summary(cpu)
```
The above are the stats about the data and we see a huge various in the data. so data need to scalled



```{r}
dim(cpu)
```

For visualise data lets add index to the data.

```{r}
ind = c(1:209)
cpu$index = ind
```


small dataset to with only 209 instance


```{r}
library(ggplot2)
ggplot(cpu, aes(x = CACH, y = Performance)) + geom_line(color= "blue")+ geom_point(color ="red")+geom_hline(yintercept = mean(cpu$Performance), color = "green") + geom_hline(yintercept = min(cpu$Performance)) +theme_bw()
```



Cache range 50 - 150 seems to have some higher perfomance. as most of the cpu are above average


```{r}
library(ggplot2)

ggplot(cpu,aes(index,Performance)) + geom_line(color = "blue", size=1)+  geom_point(color = "red") + geom_hline(yintercept = mean(cpu$Performance), color = "green") + geom_hline(yintercept = min(cpu$Performance)) +theme_bw()
```
The above is an over all performance of all the cpu. 
lets see for each vendors


```{r}
library(ggplot2)

ggplot(cpu,aes(index,Performance, color = Vendor)) + geom_line(size=0.5)+  geom_point() + geom_hline(yintercept = mean(cpu$Performance), color = "green") + geom_hline(yintercept = min(cpu$Performance)) +theme_bw()

```

From the above see vendor Adviser an wang produce CPU with high Performance.

```{r}
library(ggplot2)

ggplot(cpu,aes(index,MMAX)) + geom_line(color = "blue", size=1)+  geom_point(color = "red") + geom_hline(yintercept = mean(cpu$MMAX), color = "green") + geom_hline(yintercept = min(cpu$MMAX)) +theme_bw()
```


Overall max memory for the cpu.


```{r}
library(ggplot2)

ggplot(cpu,aes(index,MMAX, color = Vendor)) + geom_line(size=0.5)+  geom_point() + geom_hline(yintercept = mean(cpu$MMAX), color = "green") + geom_hline(yintercept = min(cpu$MMAX)) +theme_bw()
```
Adviser and wang having max memory as we expected. we can see the relation between mmax and performance.


```{r}
library(ggplot2)

ggplot(cpu,aes(index,CACH)) + geom_line(color = "blue", size=1)+  geom_point(color = "red") + geom_hline(yintercept = mean(cpu$CACH), color = "green") + geom_hline(yintercept = min(cpu$CACH)) +theme_bw()
```




```{r}
library(ggplot2)

ggplot(cpu,aes(index,CACH, color = Vendor)) + geom_line(size=0.5)+  geom_point() + geom_hline(yintercept = mean(cpu$CACH), color = "green") + geom_hline(yintercept = min(cpu$CACH)) +theme_bw()
```
we can see from above cach is not contributing to the performance of CPU.

```{r}
library(ggplot2)

ggplot(cpu,aes(index,CHMAX, color = Vendor)) + geom_line(size=0.5)+  geom_point() + geom_hline(yintercept = mean(cpu$CHMAX), color = "green") + geom_hline(yintercept = min(cpu$CHMAX)) +theme_bw()
```
we cannot come to conclusion with this stats.

Now lets fit a regression model to see how the data behave 

```{r}
boxplot(Filter(is.numeric,cpu))

```

From the above we can see almost every features has some outlier

so lets normalize the data see how it works

```{r}
calculatenorm=function(x){
  num=x-min(x)
  denom=max(x)-min(x)
  return (num/denom)
}

normalizeset= as.data.frame(lapply(Filter(is.numeric,cpu),calculatenorm))
nrow(normalizeset)

```

```{r}
boxplot(Filter(is.numeric,normalizeset))
```

```{r}
summary(normalizeset)
```
```{r}
plot(normalizeset)
```

```{r}
normalizeset = normalizeset[normalizeset$CHMAX < 0.50,]
nrow(normalizeset)
```

```{r}
boxplot(normalizeset)
```

```{r}
cor(normalizeset)
```

From the above we can see, we cannot see any feature pair are highly corelated



```{r}
normalizeset = subset(normalizeset,select = c("MMIN","MMAX" ,"CACH","CHMIN", "CHMAX", "Performance"))
```

```{r}
cputrain = normalizeset
```
```{r}
cpufit = lm(Performance ~ . , data = cputrain)
summary(cpufit)
```

From the above foolowing are the observations:
we can eliminate the CHMIN and CHMAX as they have higher P - Value
R- Square and Adj- R- Square are Very high seems like overfit


```{r}
cpufit1 = lm(Performance ~  MMIN +MMAX + CACH  , data = cputrain)
summary(cpufit1)

```

```{r}
par(mfrow = c(2, 2)) 
plot(cpufit) 
```

```{r}
par(mfrow = c(2, 2))  
plot(cpufit1) 
```





```{r}
cputrain$predicted = predict(cpufit1)
cputrain$Resudials = residuals(cpufit1)
```



```{r}
library(dplyr)
cputrain %>% select(Performance, predicted, Resudials) %>%head()

```

```{r}
library(ggplot2)
ggplot(cputrain, aes(x =  MMIN + MMAX + CACH, y = Performance )) +
  geom_point(color = "Blue") + theme_bw()
```





```{r}
ggplot(cputrain, aes(x =  MMIN + MMAX + CACH, y = Performance ))+ geom_segment(aes(xend = MMIN + MMAX + CACH, yend = predicted), alpha = .5)+geom_smooth(method = "lm", se = FALSE, color = "black") +geom_point(color = "blue")+ geom_point(aes(y = predicted), shape = 1, color =" red") 

```


The above is the difference between the actual points Blue and Predicited Points Red and the linear regression lm is show in Black color

```{r}
barplot(cputrain$Resudials)
```
From the above we can see the resudial is somewhat evenly distributed also we can find negative effict which will detect prediction with lower than the actual. which actually better than to have higher performance result and lower performance  in actual.

```{r}
ggplot(cputrain, aes(x =  MMIN + MMAX + CACH, y = Performance ))+ geom_segment(aes(xend = MMIN + MMAX + CACH, yend = predicted), alpha = .5)+geom_point(aes(color = abs(Resudials), size = abs(Resudials))) + # size also mapped
  scale_color_continuous(low = "black", high = "green") +geom_smooth(method = "lm", se = FALSE, color = "black") + geom_point(aes(y = predicted), shape = 1, color =" blue") 
```

The above plot is visuvialsation of Residuvals 



```{r}
library("magrittr")
library("tidyr")

cputrain %>% gather (-Performance,-CHMAX, -CHMIN,-predicted, -Resudials,key = "iv", value = "x" ) %>% 
  
  ggplot( aes(x =  x, y = Performance ))+ geom_segment(aes(xend = x, yend = predicted), alpha = .5)+geom_point(aes(color = abs(Resudials), size = abs(Resudials))) + # size also mapped
  scale_color_continuous(low = "black", high = "green") +geom_smooth(method = "lm", se = FALSE, color = "black") + geom_point(aes(y = predicted), shape = 1, color =" blue") + facet_grid(~ iv , scales = "free_x") 

```


From the above we can see fitting the features seperately and see how it works. all works almost similar. so linear model fitted with these feature may predict the performance of the CPU.









